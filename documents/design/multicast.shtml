#define S_DOCUMENTATION
#define S_DOCUMENTATION_DEVELOPER
#define PAGE Documentation
#define SUBPAGE Developer
#define TITLE Reliable Multicast

#include "header.shtml"

<p>
  Author: <a href="mailto:eilemann@gmail.com">Stefan Eilemann</a><br/>
  State:
<ul>
  <li>Implemented in 0.9.1</li>
  <li>Sliding ack window and early acks in svn</li>
</ul>
</p>

<h2>Overview</h2>
<p>
  This document explores the usage of multicast or broadcast to speed up
  one-to-many communication in Equalizer, i.e., the data distribution
  for <code>eq::net::Object</code>.
</p>

<h2>Requirements</h2>
<ul>
  <li>Optional usage of a many-to-one network protocols</li>
  <li>Reliable and in-order data transmission</li>
  <li>Usage of point-to-point and many-to-one communication for one data
    transfer, e.g., to update clients within the subnet and outside.</li>
  <li>High performance, target should be 80% speed when distributing to 10
    clients compared to one point-to-point transfer.</li>
  <li>Usage of many-to-one communication for:
    <ul>
      <li><code>eq::net::Object</code> mapping</li>
      <li><code>eq::net::Object::commit</code></li>
    </ul>
  </li>
</ul>

<h2>Multicast Primer</h2>
<p>
  A multicast group is defined by the IPv4 multicast address. A multicast sender
  binds and connects a socket to the multicast address in order to send data to
  all multicast receivers. A multicast receiver listens on the multicast
  address, accepts a new sender which will return a handle (file descriptor) to
  receive data from the sender.
</p>

<h2>Design</h2>
<h3>Pragmatic General Multicast (PGM)</h3>
<p>
  PGM seems like a promising protocol, but has the disadvantage of not providing
  the reliability needed for Equalizer. PGM uses a send window to buffer data,
  and a client my only request retransmissions within this send window. Whenever
  a client reads data too slowly, the local implementation disconnects this
  client from the PGM sender. We can not recover from this disconnect, since it
  implies that data was lost.
</p>

<a name="RSP"></a>
<h3>Reliable Stream Protocol (RSP)</h3>
<p>
  Equalizer implements a reliable stream protocol (RSP) using UDP multicast as a
  low-level transport. Equalizer 0.9.1 allows to use both PGM and RSP as a
  multicast protocol. We strongly recommend using RSP and will possibly retire
  PGM support in the future. RSP provides the following features:
</p>
<ul>
  <li>Full reliablity using active acks, a slow receiver will eventually slow
    down the sender</li>
  <li>High-performance implementation:</li>
  <ul>
    <li>Read and write buffering with lock-free queueing</li>
    <li>Merging of small writes</li>
    <li>Early nacks (negative acknowledgements)</li>
    <li>Early acks, sliding sender-side ack window (svn only)</li>
    <li>Nack merging</li>
  </ul>
  <li>Multicast send interface selection</li>
  <li>Event-based (Windows) or FD-based (Posix) notification mechanism</li>
</ul>
<p>
  It does not yet implement (TODO-list):
</p>
<ul>
  <li>Client connects and disconnects during active write operations</li>
  <li>Automatic MTU discovery between Windows and non-Windows hosts</li>
  <li>Infiniband support</li>
  <li>TTL selection, i.e., multicast traffic will not leave the local
    subnet</li>
</ul>
<div class="float_right">
      <a href="/documents/design/images/rspPackets.png">
        <img src="/documents/design/images/rspPackets-small.jpg" 
         alt="RSP Packet Flow"></a>
  <div class="label">RSP packet control flow</div>
</div>
<p>
  Each member of the multicast groups call <code>listen</code> on its RSP
  connection
  (<a href="/documents/Developer/API/classeq_1_1net_1_1RSPConnection.html">API
  documentation</a>). This creates an I/O thread for this multicast
  connection. The connection discovers all other members in the multicast group
  during the initialization and creates one <code>connected</code> RSPConnection
  for each group member. A new member is signaled on the listening connection,
  and can be retrieved using accept. On each process there is therefore one
  listening and n connected connections for each multicast group. Data send is
  also received locally, that is, there is one connected RSPConnection for the
  local sender.
</p>
<p>
  Each RSP connection has a number of data buffers (64 Windows/1024 others) of
  mtu size (65000 Windows/1470 others). These buffers are circulated between the
  the application and IO thread during operations, ultimately causing the
  application thread to block in <code>read</code> or <code>write</code> if no
  buffers are available. Care has been taken in the implementation to use
  lock-free queues and batching locked operations where possible. This ensures a
  high parallelism between the application and network communications.
</p>
<p>
  The RSP communication protocol follows roughly the following pseudo-code
  algorithm:
</p>
<!-- /* --><pre>
[Application thread]
  RSPConnection::write( buffer, size )
      numPackets = size / _mtu;
      for each packet
          if no free packet available
              wake up I/O thread to trigger processing
          get free packet from _appBuffers [may block]
          copy data to packet
          enqueue packet to _threadBuffers
      wake up I/O thread to trigger processing

[RSPConnection IO thread]
  RSPConnection::handleTimeout
      processOutgoing()

  RSPConnection::handleData
      handleData()
      processOutgoing()

  RSPConnection::processOutgoing
      if repeat requests pending
          send one repeat
          set timeout to 0
      else if write pending (_threadBuffers not empty)
          merge pending write buffers up to mtu size
          update packet sequence
          send one data packet
          save packet for repetion (_writeBuffers)
      if _writeBuffers not empty and _threadBuffers empty 
          (i.e. still need ack and nothing to write)
          if timeout exceeded
              send ack request
          set timeout

  RSPConnection::handleData
      if data packet in order
          try to get free packet from application thread (_threadBuffers)
          if no packet available
              drop data packet
          else
              enqueue data packet to application (_appBuffers)
              enqueue in-order saved early packets (_recvBuffers)
              update sequence
              if sequence % ack frequency
                  send ack to sender
      else if earlier packet already received (repetition for other receiver)
          drop packet
      else if early future packet
          try to get free packet from application thread (_threadBuffers)
          if no packet available
              drop data packet
          else
              save packet (_recvBuffers)
              send NAck for all packets not received before this one

  RSPConnection::handleAck
      update receiver's sequence
      if all other receiver's have later sequence
          enqueue packets up to sequence for own read operation
              _writeBuffers -> _appBuffers

  RSPConnection::handleNAck
      queue repeat requests

  RSPConnection::handleAckRequest
      if own sequence smaller than request
          send nack with all missing packets
      else
          send ack with own sequence

[Application thread]
  RSPConnection::read( buffer, size )
     while data to be read
         if no current read buffer
             get read buffer from _appBuffers [may block]
         copy data from read buffer to application memory
         if read buffer fully used
             push buffer to IO thread (_threadBuffers)
         else
             update read buffer size
</pre><!-- */ -->

<h3>Object Mapping</h3>
<div class="float_right">
  <a href="/documents/design/images/objectSync.png">
    <img src="/documents/design/images/objectSync-small.jpg" 
         alt="Object Synchronization Diagram (0.9)"/></a>
  <div class="label">Object Synchronization Diagram (0.9)</div>
</div>
<p>
  Object mapping is requested by the slave instance. The master instance does
  not know beforehand the list of slaves, and can therefore not optimize the
  object mapping in the current implementation.
</p>
<p>
  In order to overcome this limitation, there are two possibilities: A per-node
  object instance data cache or deferring the initialization be separating it
  from the object mapping.
</p>
<p>
  When using an object instance data cache, the master instance broadcasts the
  object instance data to the first slave node mapping the object. Each node
  receiving the data will enter it into its own cache. Subsequent slave nodes
  use the instance data from the cache and only need a registration handshake
  with the master instance.
</p>
<p>
  When using a delayed initialization, no data is transmitted during
  mapping. The master instance registers the slave nodes. The first slave node
  will explicitely request the initialization data at a later time (before the
  first sync?), upon which the master will broadcast the information to all
  known slaves.
</p>

<h4>Instance Cache</h4>
<div class="float_right">
  <a href="/documents/design/images/objectSyncCache.png">
    <img src="/documents/design/images/objectSyncCache-small.jpg" 
         alt="Object Synchronization Diagram using Caching"/></a>
  <div class="label">Object Synchronization Diagram using Caching</div>
</div>
<p>
  Caching instance data has a performance penalty for the cache management and
  multicast data transfer. Multicast transfer has to be carefully selected to
  not overload the network if multiple Equalizer session run within the same
  subnet. The caching algorithm needs to yield high hit rates to avoid
  re-broadcasting instance data and conservative memory usage (instance data is
  typically only needed during initialization of a new model).
</p>
<p>
  To optimize instance data broadcast, slave instance have to explicitely
  declare interest in a certain type of data. A set of objects belongs to the
  same type of data, typically all scene graph nodes of one model have the same
  type, but scene graph nodes of different models have different types. Each
  render client subscribes to instance data broadcasts of the model it is
  currently mapping, and unsubscribes after all model data has been mapped.
</p>
<!-- /* --><pre>
  ObjectCache& Session::getObjectCache();
  uint32_t Object::getType() const;
  class ObjectCache
  {
  public:
      void request( const uint32_t type );
      void ignore( const uint32_t type );

  private:
      stde::hash_map< uint32_t, NodeVector > _registrations;
  };
</pre><!-- */ -->

<h4>Delayed Initialization</h4>
<p>
  The delayed initialization decouples the registration from the initialization
  during mapping. This allows the master to send the initialization data to all
  registered slave instances on the first initialization request.
</p>
<p>
  The main issue with delayed initialization is that it does not have a lot of
  potential for the typical hierarchical data structures used in scene
  graphs. In order to register children of a given node, the node has to be
  initialized, which causes the registration and mapping to happen almost at the
  same time.
</p>

<h3>Object Commit</h3>
<p>
  During commit time, all receivers are know. The master needs to build a
  connection list containing the multi-point connection(s) to the 'local'
  clients and the point-to-point connections to the 'remote' clients.
</p>

<h2>API</h2>
<p>
  
</p>

<h2>File Format</h2>
<!-- /* --><pre>
  node
  {
      connection { type TCPIP }
      connection
      {
          type MCIP | RSP | PGM
          hostname  "239.255.42.42"
          interface "10.1.1.1"
          port 4242
      } 
  }
</pre><!-- */ -->

<h2>Implementation</h2>
<!-- /* --><pre>
  Benchmark PGM on Windows
  PGM listening connection
    o readFD is a listening socket FD
    o writeFD is a connected socket FD to group
  PGM connected connection
    o readFD is result from accept
    o writeFD is shared with listening connection

  Node::connect( peer, TCPConnection )
    search peer for PGM connection description for our PGM connection(s)
     -> send NodeID to PGM connection, creates connected connection on peer
  
  Node::_handleConnect
    accept new connection
    if new connection is PGM connection
      read peer node id from new connection
      find existing, connected node
      set new PGM connection on node

  *MasterCM::_cmdCommit
    DataOStream::enable( slaves )
      prefer and filter duplicate PGM connections 
      [Opt: Cache result in MasterCM?]

  Session::mapObject
    mapObjectNB
      lookup and pin object instance data in cache
      send SubscribeObject packet with known instance version
    mapObjectSync
      wait on SubscribeObjectReply
      retrieve and unpin pinned object instance data from cache
      Object::applyMapData( instance data )

  Session::_cmdSubsribeObject
    send SubscribeObjectSuccess
    Object::addSlave( nodeID, cachedVersionStart, cachedVersionEnd )
      send missing versions
      ...
      return first version to apply
  
  Session::_cmdInstance
    if nodeID is ours
      forward to object instance
    potentially add to cache
</pre><!-- */ -->

<h2>Restrictions</h2>
<p>
</p>

<h2>References</h2>
<ul>
  <li><a href="http://msdn.microsoft.com/en-us/library/ms740125(VS.85).aspx">Windows
      XP PGM Implementation</a> (Install Control Panel -&gt; Add or Remove
      Programs -&gt; Add or Remove Windows Components -&gt; Message
      Queueing)</li>
  <li><a href="http://developer.novell.com/wiki/index.php/OpenPGM">OpenPGM
      Implementation</a></li>
</ul>

<h2>Issues</h2>
<h3>1. How are late joins to the multicast group handled, e.g., caused by a
  layout switch?</h3>
<p>
  Resolved: The Equalizer implementation has to ensure that no application code
  is executed during node initialization and exit.
</p>
<p>
  A layout switch currently ensures this partly. The eq::Config finishes all
  frames on a layout switch in startFrame. The application and all render
  clients have to be blocked until eq::server::Config::_updateRunning is
  finished.
</p>

<h3>2. How are the 'cache enable' requests synced during rendering, e.g., when
  running in DPlex?</h3>
<p>
  Open
</p>
<p>
  Option 1: The application has to call finishAllFrames, which will cause the
  render clients to restart almost simultaneously.
</p>
<p>
  Option 2: The application can synchronize all or a subset of the clients
  without blocking the rest or the application.<br>
  The issues is what to sync: Most of the data-to-be-mapped is view-specific,
  e.g., the model, and might be shared among multiple views.
</p>

<h3>3. Which network adapter is used in multi-network hosts?</h3>
<p>
  Resolved: The 'interface' connection parameter is used to set the IP address
  or hostname of the outgoing multicast interface.
</p>
<p>
  By default, the first interface is used on Windows XP
  (<a href="http://msdn.microsoft.com/en-us/library/ms739175(VS.85).aspx">MSDN
    doc</a>), but the RM_SET_SEND_IF socket option can be used to define another
  interface by IP address
  (<a href="http://msdn.microsoft.com/en-us/library/ms738591(VS.85).aspx">MSDN
    doc</a>), probably using the interface's unicast address (to be verified).
</p>


#include "footer.shtml"
