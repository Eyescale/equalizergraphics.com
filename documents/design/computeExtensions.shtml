#define S_DOCUMENTATION
#define S_DOCUMENTATION_DEVELOPER
#define PAGE Documentation
#define SUBPAGE Developer
#define TITLE Compute Extensions

#include "header.shtml"

<p>
Author: <a href="mailto:philippe.robert@gmail.com">philippe.robert@gmail.com</a><br/>
Version: last modified date: September 10, 2009<BR>
Status: Request for discussion<BR>
Dependencies: CUDA (libcuda, libcudart)[, OpenCL]<BR>
</p>

<h2>Overview</h2>
<p>
This extension proposes some modifications to Equalizer which enable the 
integration of GPU computing tasks into the programming and execution model 
of Equalizer.
</p>
<h3>Requirements</h3>
<p>
<ul>
<li>Optional usage of CUDA [OpenCL] in Equalizer</li>
<li>Scalability of compute tasks on multi-GPU and cluster systems</li>
<li>Efficient shared GPU memory management</li>
<li>Optimised node-to-node communication</li>
</ul>
</p>
<h3>Design Aspects</h3>
<p>
Equalizer uses <b>Compounds</b> to describe the execution of the rendering 
operations. The  rendering is performed by <b>Channels</b> which are 
referenced by the compounds -- a channel belongs to a <b>Window</b> which 
again manages the GL context. 
Windows in turn are managed by a <b>Pipe</b> object which provides the 
threading logic. Usually, only one window is used per pipe. Internally, each
Window uses an <b>OSWindow</b> object which abstracts window-system specific
functionality; i.e., using WGL, glX, or AGL. Similarly, an <b>OSPipe</b> 
object is used by each pipe to handle the backend specific device
implementation.
</p>
<p>
<center>
<img src="images/computeClasses.png"><BR><BR>
Overview class diagram (aside from AGL we currently also support glX and 
WGL)
</center>
</p>
<p>
When using CUDA one host thread can execute device code on exactly one GPU 
device; i.e., a host thread may have only one CUDA device context current 
at a time. To support multiple devices multiple host threads are thus 
required. This is a perfect match to the pipe abstraction of Equalizer. 
On the other hand, since GPU computing does not perform any rendering and 
as such does not rely on a drawable, the window abstraction does not fit. 
Instead we use the pipe itself to execute the COMPUTE tasks defined in the 
compound description. This fits naturally to the design approach of both 
CUDA and OpenCL, and moreover we can avoid introducing another object 
hierarchy in parallel to the current pipe - window - channel model.
As a consequence, we can simply use specific attributes on the pipe to set 
up the <b>ComputeContext</b> that provides the interface to either CUDA or 
OpenCL.
</p>

<h2>Internal Changes</h2>

<p>
In the first phase we target C for CUDA, later we will add support for 
OpenCL. The proposed changes are related to
<ul>
  <li>the proper initialisation of each participating pipe for GPU 
      computing purposes, </li>
  <li>the synchronisation of shared (GPU) memory, </li>
  <li>the execution of CUDA kernels.</li>
</ul>
Moreover, the file format is extended to support COMPUTE tasks. There exist 
two different use-cases:
<ul>
  <li> Compute tasks are executed on a pipe which also performs OpenGL draw 
       operations; i.e. interoperability with OpenGL is enabled.</li>
  <li> Compute tasks are executed on a compute-only device, such as a Tesla 
       GPU or a designated GPU.</li>
</ul>
</p>
<h3>Initialisation</h3>
<p>
CUDA initialisation is handled by a <b>cudaComputeContext</b> object. In the
future, other contexts might be used, such as <b>clComputeContext</b> for 
OpenCL. 
The context uses the specified <b>cuda_device</b> to select the appropriate 
compute device, or 0 if none is specified explicitely. Internally, the CUDA 
device is then selected by calling cudaSetDevice(). 
If OpenGL interoperability is required the specified CUDA device has to be 
set up accordingly. This mode can be selected using the attribute 
<b>cuda_gl_interop</b> which results in a call to cudaGLSetGLDevice() using 
the appropriate device identifier. Thus, the compute context is set up
after the window (OpenGL context) initialisation.
</p>
<p>
To select the behaviour of synchronous CUDA function calls, either choose 
<b>yield</b>, <b>block</b> or <b>spin</b> for the attribute 
<b>cuda_sync_mode</b>. 
Internally this results in a call to cudaSetDeviceFlags() prior to any other
runtime calls.
</p>

<h3>Execution Model</h3>

<p>
The execution model is based on a special-purpose COMPUTE task, analogous to
the common rendering tasks. To integrate the compute tasks into the 
Equalizer execution model, pipes are referenced from the compound structure 
just as channels are used for rendering.
</p>
<p>
Equalizer provides the information required to launch a CUDA kernel as it is
done with other, rendering specific parameters, such as the viewport for 
example. In the context of CUDA based GPU computing this includes

<ul>
<li>The rank of the ComputeUnit</li>
<li>The number of blocks (grid dimension) [TBD]</li>
<li>The block parameters (block size) [TBD]</li>
</ul>

It is then up to the developer to manage the kernel-specific launch 
parameters (offset based on the rank) and data distribution. The former can 
be implemented using common frame data, the latter by using distributed 
shared memory. 
Note that we do not have to provide additional synchronisation functionality
because __syncthreads() only operates on GPU threads within a particular 
block to coordinate shared memory access on the GPU. 
</p>
<p>
Note that when the glinterop hint is specified, compute tasks can be 
scheduled before and/or after the common rendering tasks. 
</p>
<h3>Shared Memory</h3>
<p>
To coordinate global memory access in a cluster we will provide an 
additional higher-level API targeting the needs of distributed GPU 
computing applications. These additions will be part of eq::net.
The memory transfer from/to the GPU is performed implicitly by these
shared memory objects as part of the sync/dirty/commit 
cycle.
</p>

<h3>Events</h3>
<p>
To optimise the communication between participating nodes we propose an
extension of the event handling mechanism with the goal to support a more
generic producer-consumer model; i.e., to support arbitrary node-to-node 
communication.
</p>
<p>
Note that strictly speaking this extension is not part of the GPU computing 
extension itself, but it is a requirement for more complex GPU computing 
applications. The details will therefore be part of the separate 
<a href="eventHandling.shtml">event handling design document</a>.
</p>

<h2>API</h2>
<h3>Data Distribution</h3>
<!-- /* --><pre>
    class ComputeContext
    {
    };
</pre><!-- */ -->

<h2>File Format</h2>
<h3>Extension</h3>
<h3>Example</h3>
In this example 2 compute units are used, one on a compute-only device, the 
other interoperating with the rendering:
<!-- /* --><pre>
  pipe
  {
      name "gpu1"

      window
      {
          name "window"
          channel
          {
              name "channel1"
          }
      }
      attributes
      { 
          cuda_gl_interop on 
          cuda_sync_mode block
          cuda_device 0
      }
  }
  pipe
  {
      name "gpu2"

      attributes
      { 
          cuda_sync_mode block
          cuda_device 1
      }
  }
  compound
  {
      channel "channel1"

      compound
      {
          name "mixed-mode compound"
      }

      compound
      {
          name "compute compound"
      }
  }
</pre><!-- */ -->

<a name="Issues"></a>
<h2>Open Issues</h2>

<ol>
<li><b>CUDA Implementation:</b>
    Do we use the CUDA driver API or the CUDA runtime API for the 
    implementation? These APIs are mutually exclusive. 
    <BR><BR>
    For now we use the CUDA runtime API. It eases device code management by 
    providing implicit initialization, context and module management. The 
    host code generated by nvcc is based on the CUDA runtime API, so 
    applications that link to this code must use the CUDA runtime API. 
    Finally, device emulation only works for code written for the runtime 
    API.
    <BR><BR>
</li>

<li><b>Equalizer Integration:</b> 
    Do we have to introduce new abstractions as equivalent to Windows and
    Channels?
    <BR><BR>
    No. GPU compute tasks can be implemented on pipes.
    <BR><BR>
</li>

<li><b>Compound Definition:</b>
    How are pipes integrated into the compound description?
    <BR><BR>
</li>

<li><b>Dynamic Workload Balancing:</b>
    Do we provide separate timing values to balance the compute tasks, or 
    should they be integrated into the normal loadbalancing?
    <BR><BR>
</li>

<li><b>Kernel Launch Parameters:</b>
    Should the grid and block parameters be transported by Equalizer, or 
    should this be done by the programmer using regular frame data? What if 
    multiple kernels are used in one compute task, do they use the same 
    launch configuration?
    <BR><BR>
</li>

<li><b>Memory Proxies:</b>
    Should we provide special-purpose implementations of shared memory 
    proxy objects or just one abstract base class? E.g., a global array?
    <BR><BR>
</li>

<li><b>Asynchronous Memory Transfer:</b>
    How can we use asynchronous communication to hide the latency of the 
    host-device transfer?
    <BR><BR>
</li>
</ol>

#include "footer.shtml"

